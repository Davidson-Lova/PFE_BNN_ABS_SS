\documentclass{article}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% Langue
\usepackage[french]{babel}
\usepackage[T1]{fontenc}

% Redirection avec la table des matières
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

% Pour les scripts
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\title{PFE}
\author{Khaled MEDJKOUH \\ Davidson Lova RAZAFINDRAKOTO}

\begin{document}

\maketitle

\pagebreak

\tableofcontents

\pagebreak

\section{Introduction}
Incertitudes utile dans plusieurs domaines et la quantification duquel
est nécessaire pour des évaluations de risques, déctection d'anomalie et prise décision.


\subsection{Réseau de neurones}



+ Historique sur les réseaux de neurones

+ Explication des poids et biais et fonction d'activation

+ Quelque résultats et application

+ Ce qui nous amène à un problème



\subsection{Source d'incertitudes dans un réseaux de neurones}

On modélise le réseaux de neurones comme la fonction non linéaire

$$f : (x, \theta) \in \mathcal{X} \times \Theta \mapsto f(x, \theta) \in \mathcal{Y}$$

où

\begin{itemize}
    \item $\mathcal{X} \subset \mathbb{R}^{n_e}$, l'espace des variables d'entrées
    \item $\mathcal{Y} \subset \mathbb{R}^{n_s}$, l'espace des variables de sorties
    \item $\Theta \subset \mathbb{R}^{n_p}$ , l'espace des paramètres
\end{itemize}

On se donne maintenant une base de données d'entrainement
$D = \{ (x_i, y_i)\}_{i = 1}^N \in (\mathcal{X} \times \mathcal{Y})^N$.

\subsubsection{Acquisition des données}
Si on prend comme données d'entrées $x$ une mesure d'une quantité réel $\tilde{x}$.

Il y a une variabilité sur la mesure en fonction des circonstances
et conditions $\omega \in \Omega$ dans lequel la mesure a été effectué.

La sortie $y$ peut aussi subir des erreurs de labelisation
(pour le cas d'une tâche de classification) ou aussi de mesure si c'est une quantité mesuré.

En somme on a une incertitude sur l'entrée $x | \omega \sim p_{x | \omega}$
et $y | \omega \sim p_{y | \omega}$.

S'ajoute à ça, l'incertitude sur $x$ qui peut se propager sur le $y$.

\subsubsection{Structure du model}

Les paramètres $\theta$ et donc l'espace $\Theta$ est variable en fonction du choix de model $s$.

On a $\theta | D,s \sim p_{\theta | D, s}$

\subsection{Prédiction}

La distribution de la prédiction $y^*$ sachant une entrée $x^*$ est données par

$$ p(y^* | x^*, D) = \int_{\Theta} \underbrace{p(y^* | x^*, \theta)}_{\text{Données}} \underbrace{p(\theta  | D)}_{\text{Modèle}} d \theta$$

\subsubsection{Incertitude aléatoire}

Insert definition here

Elle affecte la partie $p(y^* | x^*, \theta)$ de la tâche de prédiction,
elle est dus à la variabilité(précision et erreurs) lors des mesures.
Elle est donnée et inhérent au problème.

\subsubsection{Incertitude épistemique}

Insert definition here

Affecte la partie $p(\theta  | D)$ de la tâche de prédiction,
elle due :

\begin{itemize}
    \item à la compléxité du modèle,
    \item aux erreurs durant la phase d'entrainement,
    \item à la manque d'information à cause de données manquantes
          ou la capacité de représentation des donnés d'entrainement.
\end{itemize}

\pagebreak

\section{Réseau de neurones bayésiens (BNN)}

% + Présentation de réseaux de neurones bayésiens

Les réseaux de neurones bayésiens sont des réseaux de neurones
dont les poids sont, non pas des quantités déterministes (comme
dans le cas d'un NN normale) mais des distributions.

A l'initialisation, les poids suivent une loi a priori $p(\theta)$,
et l'entrainement consiste à évaluer l'a posteriori de cette loi conditionnée
aux données d'entrainement $p(\theta | D)$.

% + Donner son interet dans la résolution du problème précédent

% + Donner les méthodes d'entrainement d'un tel réseaux
On ne dispose pas dans le cas générale, d'une formule analytique de cette
distribution a posteriori.

Voici trois familles de méthodes pour approcher cette distribution :


\begin{itemize}
    \item Méthodes variationelle
    \item Méthodes par échantillonage ou
          Monte Carlo (qu'on va voir dans la suite)
    \item Méthodes de Laplace
\end{itemize}


\subsection{Méthodes variationelles}

Ici on approche $p(\theta | D)$ par une famille de distribution
paramétrique $\{q^{\gamma}(\theta)\}_{\gamma}$ (souvent des gaussiennes).
Le but est de choisir $\gamma$ qui rapproche $q^{\gamma}(\theta)$
le plus de $p(\theta | D)$. La distance choisit ici est la divergence
de Kullback-Leibler :

$$KL(q||p) = \mathbb{E}_q \left[\log \frac{q^{\gamma}(\theta)}{p(\theta | D)}\right]$$

Comme on ne connait pas $p(\theta | D)$, on utilise l'ELBO (evidence lower bound)
qui est égal à la divergence à une constante paramètres

$$L =\mathbb{E}_q \left[\log \frac{p(y | D, \theta)}{q^{\gamma}(\theta)}\right]$$

(on a en effet $KL(q||p) = -L +  \log p(y| D)$)

\subsection{Méthode de Laplace}

$\hat{\theta}$ l'estimateur de maximum d'a priori

$$\log p(\theta | D) \approx \log p(\hat{\theta} | D)
    + \frac{1}{2} (\theta - \hat{\theta})^T (H + \tau I)
    (\theta - \hat{\theta})$$

$$p(\theta | D) \sim \mathcal{N}(\hat{\theta}, (H + \tau I)^{-1})$$

\subsection{Méthodes par échantillonage ou Monte Carlo}

La formule de Bayes nous donne

$$p(\theta | D) = \frac{p(D | \theta) }{p(D)}p(\theta)$$

\begin{itemize}
    \item $p(D | \theta)$ la vraissemblance des données $D$ sachant le paramètre $\theta$,
    \item $p(\theta)$ la distribution a priori de $\theta$,
    \item $p(D)$ la distribution des données d'entrainement.
\end{itemize}

$$p(\theta | D) \propto $$

\pagebreak

\section{Algorithme BNN-ABC-SS}

\subsection{ABC (Approximate Bayesian Computation)}

La méthode ABC consiste à evaluer $p(\theta | D)$ sans évaluer
la vraissemblance qui peut s'avérer couteux.

Posons $\hat{y} = f(x, \theta)$ la sortie d'une évaluation de $x$ par 
réseaux de neurones $f$ avec paramètre $\theta$. 

La formule de Bayes donne 

$$ p(\theta, \hat{y} | D) \propto p(D | \hat{y}, \theta) p(\hat{y} | \theta)
p(\theta)$$

Pour simuler selon la distribution du second membre, on applique
l'Algorithme de rejet.

\subsection{SS (Subset Simulation)}

+ celui dans le rapport d'avant

\subsection{Pseudo - Code}

+ celui dans le rapprot d'avant

\pagebreak

\section{Réalisation}

+ On se donne un base de données à étudier
avec une comparasion avec des méthodes déja existantes

\subsection{Cosinus perturbé}

\subsection{Sinus perturbé}

\section{Conclusion et perspective}

\end{document}